# Sistema de entrenamiento de IA para evaluación de batallas de DJ
# Utiliza TensorFlow/PyTorch para entrenar modelos de evaluación automática

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
import librosa
import json
import os
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import threading
import queue

class DJBattleAITrainer:
    def __init__(self, config=None):
        self.config = config or {
            'sample_rate': 44100,
            'n_mels': 128,
            'n_fft': 2048,
            'hop_length': 512,
            'max_duration': 30,  # segundos
            'model_save_path': './models',
            'data_path': './training_data',
            'batch_size': 32,\n            'epochs': 100,\n            'learning_rate': 0.001,\n            'validation_split': 0.2\n        }\n        \n        self.feature_extractors = {\n            'technical': TechnicalFeatureExtractor(),\n            'musical': MusicalFeatureExtractor(),\n            'transition': TransitionFeatureExtractor(),\n            'crowd': CrowdResponseExtractor()\n        }\n        \n        self.models = {}\n        self.scalers = {}\n        self.training_history = {}\n        \n        self.setup_models()\n    \n    def setup_models(self):\n        \"\"\"Configurar diferentes modelos de evaluación\"\"\"\n        \n        # Modelo para evaluación técnica (BPM matching, pitch accuracy)\n        self.models['technical'] = self.create_technical_model()\n        \n        # Modelo para evaluación musical (harmonic mixing, key compatibility)\n        self.models['musical'] = self.create_musical_model()\n        \n        # Modelo para evaluación de transiciones\n        self.models['transition'] = self.create_transition_model()\n        \n        # Modelo para respuesta de multitud\n        self.models['crowd'] = self.create_crowd_model()\n        \n        # Modelo ensemble para puntuación final\n        self.models['ensemble'] = self.create_ensemble_model()\n    \n    def create_technical_model(self):\n        \"\"\"Modelo CNN-LSTM para análisis técnico\"\"\"\n        \n        model = models.Sequential([\n            # Capas convolucionales para análisis espectral\n            layers.Conv2D(32, (3, 3), activation='relu', \n                         input_shape=(self.config['n_mels'], 130, 1)),\n            layers.BatchNormalization(),\n            layers.Conv2D(64, (3, 3), activation='relu'),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D((2, 2)),\n            \n            layers.Conv2D(128, (3, 3), activation='relu'),\n            layers.BatchNormalization(),\n            layers.Conv2D(256, (3, 3), activation='relu'),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D((2, 2)),\n            \n            # Reshape para LSTM\n            layers.Reshape((-1, 256)),\n            \n            # Capas LSTM para análisis temporal\n            layers.LSTM(128, return_sequences=True),\n            layers.Dropout(0.3),\n            layers.LSTM(64),\n            layers.Dropout(0.3),\n            \n            # Capas densas para clasificación\n            layers.Dense(128, activation='relu'),\n            layers.Dropout(0.4),\n            layers.Dense(64, activation='relu'),\n            layers.Dense(32, activation='relu'),\n            \n            # Salidas múltiples para diferentes métricas técnicas\n            layers.Dense(8, activation='sigmoid', name='technical_scores')\n        ])\n        \n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=self.config['learning_rate']),\n            loss='mse',\n            metrics=['mae', 'accuracy']\n        )\n        \n        return model\n    \n    def create_musical_model(self):\n        \"\"\"Modelo para análisis musical y armónico\"\"\"\n        \n        # Input para características cromáticas\n        chroma_input = layers.Input(shape=(12,), name='chroma_input')\n        \n        # Input para características tonales\n        tonal_input = layers.Input(shape=(6,), name='tonal_input')\n        \n        # Input para características rítmicas\n        rhythm_input = layers.Input(shape=(8,), name='rhythm_input')\n        \n        # Procesamiento de características cromáticas\n        chroma_dense = layers.Dense(24, activation='relu')(chroma_input)\n        chroma_dense = layers.Dropout(0.2)(chroma_dense)\n        chroma_dense = layers.Dense(12, activation='relu')(chroma_dense)\n        \n        # Procesamiento de características tonales\n        tonal_dense = layers.Dense(12, activation='relu')(tonal_input)\n        tonal_dense = layers.Dropout(0.2)(tonal_dense)\n        \n        # Procesamiento de características rítmicas\n        rhythm_dense = layers.Dense(16, activation='relu')(rhythm_input)\n        rhythm_dense = layers.Dropout(0.2)(rhythm_dense)\n        \n        # Concatenar todas las características\n        combined = layers.Concatenate()([chroma_dense, tonal_dense, rhythm_dense])\n        \n        # Capas finales\n        combined = layers.Dense(64, activation='relu')(combined)\n        combined = layers.Dropout(0.3)(combined)\n        combined = layers.Dense(32, activation='relu')(combined)\n        \n        # Salida para compatibilidad musical\n        output = layers.Dense(5, activation='sigmoid', name='musical_scores')(combined)\n        \n        model = models.Model(\n            inputs=[chroma_input, tonal_input, rhythm_input],\n            outputs=output\n        )\n        \n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=self.config['learning_rate']),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        return model\n    \n    def create_transition_model(self):\n        \"\"\"Modelo especializado en análisis de transiciones\"\"\"\n        \n        # Arquitectura de red siamesa para comparar dos segmentos\n        input_shape = (self.config['n_mels'], 65, 1)  # Segmentos más cortos\n        \n        def create_branch():\n            branch = models.Sequential([\n                layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n                layers.BatchNormalization(),\n                layers.Conv2D(64, (3, 3), activation='relu'),\n                layers.BatchNormalization(),\n                layers.MaxPooling2D((2, 2)),\n                \n                layers.Conv2D(128, (3, 3), activation='relu'),\n                layers.BatchNormalization(),\n                layers.GlobalAveragePooling2D(),\n                \n                layers.Dense(256, activation='relu'),\n                layers.Dropout(0.3),\n                layers.Dense(128, activation='relu')\n            ])\n            return branch\n        \n        # Dos ramas para los segmentos antes y después de la transición\n        branch = create_branch()\n        \n        input_a = layers.Input(shape=input_shape, name='segment_before')\n        input_b = layers.Input(shape=input_shape, name='segment_after')\n        \n        encoded_a = branch(input_a)\n        encoded_b = branch(input_b)\n        \n        # Calcular distancia y similitud\n        distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_a, encoded_b])\n        similarity = layers.Lambda(lambda x: x[0] * x[1])([encoded_a, encoded_b])\n        \n        # Combinar características\n        combined = layers.Concatenate()([distance, similarity, encoded_a, encoded_b])\n        \n        # Capas finales para evaluación de transición\n        combined = layers.Dense(256, activation='relu')(combined)\n        combined = layers.Dropout(0.4)(combined)\n        combined = layers.Dense(128, activation='relu')(combined)\n        combined = layers.Dense(64, activation='relu')(combined)\n        \n        # Salida: calidad de transición (0-1)\n        output = layers.Dense(1, activation='sigmoid', name='transition_quality')(combined)\n        \n        model = models.Model(inputs=[input_a, input_b], outputs=output)\n        \n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=self.config['learning_rate']),\n            loss='binary_crossentropy',\n            metrics=['accuracy', 'precision', 'recall']\n        )\n        \n        return model\n    \n    def create_crowd_model(self):\n        \"\"\"Modelo para predecir respuesta de la audiencia\"\"\"\n        \n        model = models.Sequential([\n            # Entrada: características de audio + metadatos\n            layers.Input(shape=(50,)),  # Características combinadas\n            \n            layers.Dense(128, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            \n            layers.Dense(64, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            \n            layers.Dense(32, activation='relu'),\n            layers.Dropout(0.2),\n            \n            # Salida: nivel de energía/respuesta de la multitud\n            layers.Dense(1, activation='sigmoid', name='crowd_response')\n        ])\n        \n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=self.config['learning_rate']),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        return model\n    \n    def create_ensemble_model(self):\n        \"\"\"Modelo ensemble que combina todas las evaluaciones\"\"\"\n        \n        # Inputs de otros modelos\n        technical_input = layers.Input(shape=(8,), name='technical_features')\n        musical_input = layers.Input(shape=(5,), name='musical_features')\n        transition_input = layers.Input(shape=(1,), name='transition_features')\n        crowd_input = layers.Input(shape=(1,), name='crowd_features')\n        \n        # Pesos aprendibles para cada componente\n        technical_weighted = layers.Dense(8, activation='relu')(technical_input)\n        musical_weighted = layers.Dense(5, activation='relu')(musical_input)\n        transition_weighted = layers.Dense(4, activation='relu')(transition_input)\n        crowd_weighted = layers.Dense(2, activation='relu')(crowd_input)\n        \n        # Combinar todos los componentes\n        combined = layers.Concatenate()([\n            technical_weighted, musical_weighted, \n            transition_weighted, crowd_weighted\n        ])\n        \n        # Capas de decisión final\n        combined = layers.Dense(32, activation='relu')(combined)\n        combined = layers.Dropout(0.3)(combined)\n        combined = layers.Dense(16, activation='relu')(combined)\n        \n        # Puntuación final (0-100)\n        final_score = layers.Dense(1, activation='linear', name='final_score')(combined)\n        \n        # Ranking categórico (Amateur, Intermediate, Professional, Expert)\n        ranking = layers.Dense(4, activation='softmax', name='skill_ranking')(combined)\n        \n        model = models.Model(\n            inputs=[technical_input, musical_input, transition_input, crowd_input],\n            outputs=[final_score, ranking]\n        )\n        \n        model.compile(\n            optimizer=optimizers.Adam(learning_rate=self.config['learning_rate']),\n            loss={\n                'final_score': 'mse',\n                'skill_ranking': 'categorical_crossentropy'\n            },\n            metrics={\n                'final_score': ['mae'],\n                'skill_ranking': ['accuracy']\n            },\n            loss_weights={'final_score': 0.7, 'skill_ranking': 0.3}\n        )\n        \n        return model\n    \n    def prepare_training_data(self, data_directory):\n        \"\"\"Preparar datos de entrenamiento desde archivos de audio y metadatos\"\"\"\n        \n        print(\"📊 Preparando datos de entrenamiento...\")\n        \n        # Cargar datos existentes o crear nuevos\n        training_data = {\n            'technical': {'X': [], 'y': []},\n            'musical': {'X': [], 'y': []},\n            'transition': {'X': [], 'y': []},\n            'crowd': {'X': [], 'y': []}\n        }\n        \n        # Procesar archivos de audio\n        audio_files = [f for f in os.listdir(data_directory) if f.endswith(('.wav', '.mp3'))]\n        \n        for audio_file in audio_files:\n            try {\n                file_path = os.path.join(data_directory, audio_file)\n                metadata_file = file_path.replace('.wav', '.json').replace('.mp3', '.json')\n                \n                if not os.path.exists(metadata_file):\n                    continue\n                \n                # Cargar audio\n                audio, sr = librosa.load(file_path, sr=self.config['sample_rate'])\n                \n                # Cargar metadatos (evaluaciones humanas)\n                with open(metadata_file, 'r') as f:\n                    metadata = json.load(f)\n                \n                # Extraer características para cada modelo\n                features = self.extract_all_features(audio, sr, metadata)\n                \n                # Agregar a datasets de entrenamiento\n                for model_type, data in features.items():\n                    if data['X'] is not None and data['y'] is not None:\n                        training_data[model_type]['X'].append(data['X'])\n                        training_data[model_type]['y'].append(data['y'])\n                \n                print(f\"✅ Procesado: {audio_file}\")\n                \n            } catch Exception as e:\n                print(f\"❌ Error procesando {audio_file}: {str(e)}\")\n                continue\n        \n        # Convertir a arrays numpy\n        for model_type in training_data:\n            if training_data[model_type]['X']:\n                training_data[model_type]['X'] = np.array(training_data[model_type]['X'])\n                training_data[model_type]['y'] = np.array(training_data[model_type]['y'])\n            else:\n                print(f\"⚠️ No hay datos para el modelo {model_type}\")\n        \n        return training_data\n    \n    def extract_all_features(self, audio, sr, metadata):\n        \"\"\"Extraer todas las características necesarias para el entrenamiento\"\"\"\n        \n        features = {}\n        \n        try {\n            # Características técnicas\n            technical_features = self.feature_extractors['technical'].extract(audio, sr)\n            technical_labels = metadata.get('technical_scores', np.zeros(8))\n            features['technical'] = {\n                'X': technical_features,\n                'y': technical_labels\n            }\n            \n            # Características musicales\n            musical_features = self.feature_extractors['musical'].extract(audio, sr)\n            musical_labels = metadata.get('musical_scores', np.zeros(5))\n            features['musical'] = {\n                'X': musical_features,\n                'y': musical_labels\n            }\n            \n            # Características de transición\n            transition_features = self.feature_extractors['transition'].extract(audio, sr)\n            transition_labels = metadata.get('transition_quality', 0.5)\n            features['transition'] = {\n                'X': transition_features,\n                'y': transition_labels\n            }\n            \n            # Características de respuesta de multitud\n            crowd_features = self.feature_extractors['crowd'].extract(audio, sr)\n            crowd_labels = metadata.get('crowd_response', 0.5)\n            features['crowd'] = {\n                'X': crowd_features,\n                'y': crowd_labels\n            }\n            \n        } except Exception as e:\n            print(f\"❌ Error extrayendo características: {str(e)}\")\n            for model_type in ['technical', 'musical', 'transition', 'crowd']:\n                features[model_type] = {'X': None, 'y': None}\n        \n        return features\n    \n    def train_all_models(self, training_data):\n        \"\"\"Entrenar todos los modelos\"\"\"\n        \n        print(\"🚀 Iniciando entrenamiento de modelos...\")\n        \n        for model_name in ['technical', 'musical', 'transition', 'crowd']:\n            if model_name in training_data and len(training_data[model_name]['X']) > 0:\n                print(f\"\\n🔥 Entrenando modelo {model_name}...\")\n                self.train_single_model(model_name, training_data[model_name])\n            else:\n                print(f\"⚠️ Saltando {model_name} - datos insuficientes\")\n        \n        # Entrenar modelo ensemble después de los individuales\n        self.train_ensemble_model(training_data)\n    \n    def train_single_model(self, model_name, data):\n        \"\"\"Entrenar un modelo individual\"\"\"\n        \n        X, y = data['X'], data['y']\n        \n        # Dividir datos\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=self.config['validation_split'], random_state=42\n        )\n        \n        # Normalizar datos si es necesario\n        if model_name in ['musical', 'crowd']:\n            scaler = StandardScaler()\n            X_train = scaler.fit_transform(X_train)\n            X_val = scaler.transform(X_val)\n            self.scalers[model_name] = scaler\n        \n        # Callbacks\n        callbacks_list = [\n            callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=15,\n                restore_best_weights=True\n            ),\n            callbacks.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.5,\n                patience=8,\n                min_lr=1e-7\n            ),\n            callbacks.ModelCheckpoint(\n                filepath=f\"{self.config['model_save_path']}/{model_name}_best.h5\",\n                monitor='val_loss',\n                save_best_only=True\n            )\n        ]\n        \n        # Entrenar modelo\n        history = self.models[model_name].fit(\n            X_train, y_train,\n            batch_size=self.config['batch_size'],\n            epochs=self.config['epochs'],\n            validation_data=(X_val, y_val),\n            callbacks=callbacks_list,\n            verbose=1\n        )\n        \n        self.training_history[model_name] = history.history\n        \n        # Evaluar modelo\n        self.evaluate_model(model_name, X_val, y_val)\n    \n    def evaluate_model(self, model_name, X_val, y_val):\n        \"\"\"Evaluar rendimiento del modelo\"\"\"\n        \n        print(f\"\\n📊 Evaluando modelo {model_name}...\")\n        \n        # Predicciones\n        predictions = self.models[model_name].predict(X_val)\n        \n        # Métricas\n        if model_name == 'transition':\n            # Clasificación binaria\n            pred_binary = (predictions > 0.5).astype(int)\n            y_binary = (y_val > 0.5).astype(int)\n            \n            print(classification_report(y_binary, pred_binary))\n        else:\n            # Regresión\n            mse = np.mean((predictions - y_val) ** 2)\n            mae = np.mean(np.abs(predictions - y_val))\n            \n            print(f\"MSE: {mse:.4f}\")\n            print(f\"MAE: {mae:.4f}\")\n        \n        # Guardar métricas\n        metrics_file = f\"{self.config['model_save_path']}/{model_name}_metrics.json\"\n        with open(metrics_file, 'w') as f:\n            json.dump({\n                'mse': float(mse) if 'mse' in locals() else None,\n                'mae': float(mae) if 'mae' in locals() else None,\n                'training_history': self.training_history.get(model_name, {})\n            }, f, indent=2)\n    \n    def train_ensemble_model(self, training_data):\n        \"\"\"Entrenar el modelo ensemble\"\"\"\n        \n        print(\"\\n🎯 Entrenando modelo ensemble...\")\n        \n        # Generar predicciones de modelos base para entrenamiento del ensemble\n        ensemble_X, ensemble_y = self.prepare_ensemble_data(training_data)\n        \n        if ensemble_X is None:\n            print(\"⚠️ No se puede entrenar ensemble - datos insuficientes\")\n            return\n        \n        # Entrenar ensemble\n        X_train, X_val, y_train, y_val = train_test_split(\n            ensemble_X, ensemble_y, test_size=0.2, random_state=42\n        )\n        \n        history = self.models['ensemble'].fit(\n            X_train, y_train,\n            batch_size=self.config['batch_size'],\n            epochs=50,  # Menos épocas para ensemble\n            validation_data=(X_val, y_val),\n            verbose=1\n        )\n        \n        self.training_history['ensemble'] = history.history\n    \n    def save_models(self):\n        \"\"\"Guardar todos los modelos entrenados\"\"\"\n        \n        print(\"💾 Guardando modelos...\")\n        \n        # Crear directorio si no existe\n        os.makedirs(self.config['model_save_path'], exist_ok=True)\n        \n        # Guardar modelos\n        for model_name, model in self.models.items():\n            model_path = f\"{self.config['model_save_path']}/{model_name}_model.h5\"\n            model.save(model_path)\n            print(f\"✅ Guardado: {model_path}\")\n        \n        # Guardar scalers\n        for scaler_name, scaler in self.scalers.items():\n            scaler_path = f\"{self.config['model_save_path']}/{scaler_name}_scaler.pkl\"\n            with open(scaler_path, 'wb') as f:\n                pickle.dump(scaler, f)\n        \n        # Guardar configuración\n        config_path = f\"{self.config['model_save_path']}/training_config.json\"\n        with open(config_path, 'w') as f:\n            json.dump(self.config, f, indent=2)\n        \n        print(\"✅ Todos los modelos guardados exitosamente\")\n    \n    def load_models(self, model_path=None):\n        \"\"\"Cargar modelos preentrenados\"\"\"\n        \n        model_path = model_path or self.config['model_save_path']\n        \n        print(f\"📁 Cargando modelos desde {model_path}...\")\n        \n        try:\n            for model_name in self.models.keys():\n                model_file = f\"{model_path}/{model_name}_model.h5\"\n                if os.path.exists(model_file):\n                    self.models[model_name] = tf.keras.models.load_model(model_file)\n                    print(f\"✅ Cargado: {model_name}\")\n                \n                # Cargar scaler si existe\n                scaler_file = f\"{model_path}/{model_name}_scaler.pkl\"\n                if os.path.exists(scaler_file):\n                    with open(scaler_file, 'rb') as f:\n                        self.scalers[model_name] = pickle.load(f)\n            \n            print(\"✅ Modelos cargados exitosamente\")\n            \n        } except Exception as e:\n            print(f\"❌ Error cargando modelos: {str(e)}\")\n    \n    # Método principal de entrenamiento\n    def train(self, data_directory):\n        \"\"\"Proceso completo de entrenamiento\"\"\"\n        \n        print(\"🎯 Iniciando entrenamiento completo del sistema IA...\")\n        \n        # Preparar datos\n        training_data = self.prepare_training_data(data_directory)\n        \n        # Entrenar modelos\n        self.train_all_models(training_data)\n        \n        # Guardar modelos\n        self.save_models()\n        \n        print(\"🎉 Entrenamiento completo finalizado!\")\n\n\n# Clases auxiliares para extracción de características\n\nclass TechnicalFeatureExtractor:\n    def extract(self, audio, sr):\n        \"\"\"Extraer características técnicas\"\"\"\n        \n        # Melspectrogram para análisis espectral\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio, sr=sr, n_mels=128, fmax=8000\n        )\n        mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Normalizar y redimensionar\n        mel_db = (mel_db - mel_db.mean()) / mel_db.std()\n        \n        # Asegurar tamaño fijo\n        target_frames = 130\n        if mel_db.shape[1] < target_frames:\n            # Pad con ceros\n            pad_width = target_frames - mel_db.shape[1]\n            mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n        else:\n            # Truncar\n            mel_db = mel_db[:, :target_frames]\n        \n        return mel_db.reshape(128, 130, 1)\n\nclass MusicalFeatureExtractor:\n    def extract(self, audio, sr):\n        \"\"\"Extraer características musicales\"\"\"\n        \n        # Chroma features\n        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n        chroma_mean = np.mean(chroma, axis=1)\n        \n        # Tonal features\n        tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)\n        tonnetz_mean = np.mean(tonnetz, axis=1)\n        \n        # Tempo y rhythm features\n        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n        tempo_features = [\n            tempo,\n            len(beats) / (len(audio) / sr),  # beats per second\n            np.std(np.diff(beats)) if len(beats) > 1 else 0,  # tempo variability\n        ]\n        \n        # Spectral features\n        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n        zcr = librosa.feature.zero_crossing_rate(audio)\n        \n        rhythm_features = [\n            np.mean(spectral_centroids),\n            np.std(spectral_centroids),\n            np.mean(spectral_rolloff),\n            np.std(spectral_rolloff),\n            np.mean(zcr)\n        ]\n        \n        return [chroma_mean, tonnetz_mean, tempo_features + rhythm_features]\n\nclass TransitionFeatureExtractor:\n    def extract(self, audio, sr):\n        \"\"\"Extraer características de transiciones\"\"\"\n        \n        # Dividir audio en segmentos para análisis de transiciones\n        segment_length = sr * 15  # 15 segundos\n        \n        if len(audio) < segment_length * 2:\n            # Audio muy corto, usar padding\n            audio = np.pad(audio, (0, segment_length * 2 - len(audio)), mode='constant')\n        \n        # Tomar dos segmentos consecutivos\n        mid_point = len(audio) // 2\n        segment1 = audio[mid_point - segment_length//2:mid_point + segment_length//2]\n        segment2 = audio[mid_point:mid_point + segment_length]\n        \n        # Extraer mel spectrograms para ambos segmentos\n        mel1 = librosa.feature.melspectrogram(y=segment1, sr=sr, n_mels=128)\n        mel2 = librosa.feature.melspectrogram(y=segment2, sr=sr, n_mels=128)\n        \n        # Normalizar\n        mel1_db = librosa.power_to_db(mel1, ref=np.max)\n        mel2_db = librosa.power_to_db(mel2, ref=np.max)\n        \n        # Redimensionar a tamaño fijo\n        target_frames = 65\n        for mel_db in [mel1_db, mel2_db]:\n            if mel_db.shape[1] < target_frames:\n                pad_width = target_frames - mel_db.shape[1]\n                mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n            else:\n                mel_db = mel_db[:, :target_frames]\n        \n        return [mel1_db.reshape(128, 65, 1), mel2_db.reshape(128, 65, 1)]\n\nclass CrowdResponseExtractor:\n    def extract(self, audio, sr):\n        \"\"\"Extraer características relacionadas con respuesta de multitud\"\"\"\n        \n        # Características de energía\n        rms = librosa.feature.rms(y=audio)\n        energy_mean = np.mean(rms)\n        energy_std = np.std(rms)\n        energy_max = np.max(rms)\n        \n        # Características espectrales\n        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n        \n        # Características de tempo\n        tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n        \n        # MFCC para características tímbricas\n        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n        mfcc_means = np.mean(mfccs, axis=1)\n        \n        # Características de contraste espectral\n        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n        contrast_mean = np.mean(contrast, axis=1)\n        \n        # Combinar todas las características\n        features = np.concatenate([\n            [energy_mean, energy_std, energy_max],\n            [np.mean(spectral_centroids), np.std(spectral_centroids)],\n            [np.mean(spectral_rolloff), np.std(spectral_rolloff)],\n            [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)],\n            [tempo],\n            mfcc_means,\n            contrast_mean\n        ])\n        \n        return features\n\nif __name__ == \"__main__\":\n    # Ejemplo de uso\n    trainer = DJBattleAITrainer()\n    \n    # Entrenar con datos de ejemplo\n    # trainer.train('./training_data')\n    \n    print(\"🎯 Sistema de entrenamiento IA listo para usar\")"