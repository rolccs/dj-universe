#!/usr/bin/env python3
"""
AI-DJ Enhanced System - Hybrid Architecture
===========================================

Sistema hÃ­brido que combina:
1. OBSIDIAN-Neural (Stable Audio Open) para generaciÃ³n base
2. LLM APIs (OpenAI/Claude/DeepSeek/Groq) para prompts inteligentes
3. DJ Universe modelos (42+) para enhancement y anÃ¡lisis
4. DUMP protocol para comunicaciÃ³n ultra-rÃ¡pida

Reemplaza completamente el Gemma-3-4B pesado con soluciÃ³n ligera y escalable
"""

import asyncio
import json
import logging
import numpy as np
import torch
import librosa
import soundfile as sf
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import tempfile
import os
from datetime import datetime

from .obsidian_llm_integration import get_obsidian_llm
from .llm_api_manager import get_llm_manager

# Import existing DJ Universe components
import sys
sys.path.append('/home/fat32/dj-universe/ai-training')
from dump_master_ai import DJUniverseMasterAI

logger = logging.getLogger(__name__)

class AIDJEnhancedSystem:
    """
    Sistema AI-DJ mejorado con arquitectura hÃ­brida
    
    Combina lo mejor de OBSIDIAN-Neural + LLM APIs + DJ Universe AI
    """
    
    def __init__(self, models_path: str = "/home/fat32/dj-universe/thunder_models"):
        self.models_path = Path(models_path)
        
        # Core components
        self.obsidian_llm = None
        self.llm_manager = None
        self.dj_master_ai = None
        
        # OBSIDIAN-Neural components (for music generation)
        self.music_generator = None
        self.layer_manager = None
        self.stems_manager = None
        
        # Model cache and optimization
        self.model_cache = {}
        self.max_cached_models = 5
        
        # Performance tracking
        self.generation_stats = {
            "total_generations": 0,
            "successful_generations": 0,
            "avg_generation_time": 0.0,
            "provider_usage": {}
        }
        
        # DJ-specific optimizations
        self.bpm_ranges = {
            "slow": (60, 90),
            "medium": (90, 130),
            "fast": (130, 180),
            "ultra_fast": (180, 200)
        }
        
        self.key_mappings = {
            "camelot_wheel": {
                "1A": "Aâ™­m", "1B": "B", "2A": "Eâ™­m", "2B": "Fâ™¯",
                "3A": "Bâ™­m", "3B": "Dâ™­", "4A": "Fm", "4B": "Aâ™­",
                "5A": "Cm", "5B": "Eâ™­", "6A": "Gm", "6B": "Bâ™­",
                "7A": "Dm", "7B": "F", "8A": "Am", "8B": "C",
                "9A": "Em", "9B": "G", "10A": "Bm", "10B": "D",
                "11A": "Fâ™¯m", "11B": "A", "12A": "Câ™¯m", "12B": "E"
            }
        }
    
    async def initialize(self):
        """Initialize all components"""
        logger.info("ðŸš€ Initializing AI-DJ Enhanced System...")
        
        # Initialize LLM components
        self.obsidian_llm = await get_obsidian_llm()
        self.llm_manager = await get_llm_manager()
        
        # Initialize DJ Universe Master AI
        self.dj_master_ai = DJUniverseMasterAI(
            models_path=str(self.models_path),
            enable_auto_repair=True,
            enable_memory=True
        )
        await self.dj_master_ai.initialize()
        
        # Initialize OBSIDIAN-Neural components (lazy loading)
        self._setup_generation_pipeline()
        
        logger.info("âœ… AI-DJ Enhanced System initialized successfully")
    
    def _setup_generation_pipeline(self):
        """Setup music generation pipeline"""
        # This will be initialized on first use to save memory
        self._generation_pipeline_ready = False
    
    async def _ensure_generation_pipeline(self):
        """Ensure music generation pipeline is ready"""
        if not self._generation_pipeline_ready:
            logger.info("âš¡ Initializing music generation pipeline...")
            
            # Import and initialize OBSIDIAN-Neural components
            try:
                # These would be imported from the actual OBSIDIAN-Neural system
                # For now, we'll create placeholder implementations
                self.music_generator = MusicGeneratorWrapper()
                self.layer_manager = LayerManagerWrapper()
                self.stems_manager = StemsManagerWrapper()
                
                await self.music_generator.initialize()
                
                self._generation_pipeline_ready = True
                logger.info("âœ… Music generation pipeline ready")
                
            except Exception as e:
                logger.error(f"âŒ Failed to initialize generation pipeline: {str(e)}")
                raise
    
    async def generate_music_realtime(self, prompt: str, bpm: float = 120.0, 
                                    key: str = "C", genre: str = "",
                                    duration: float = 30.0,
                                    enhance_with_dj_models: bool = True) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Generate music in real-time using hybrid AI approach
        
        Flow:
        1. LLM APIs enhance the prompt
        2. OBSIDIAN-Neural generates base audio
        3. DJ Universe models enhance the result
        4. Return optimized audio for DJ use
        """
        start_time = datetime.now()
        
        try:
            # Step 1: Enhance prompt using LLM APIs
            logger.info(f"ðŸŽµ Generating music: '{prompt}' | {bpm} BPM | {key} | {genre}")
            
            enhanced_prompt = await self.obsidian_llm.generate_music_prompt(
                original_prompt=prompt,
                bpm=bpm,
                key=key,
                genre=genre,
                duration=duration
            )
            
            logger.info(f"âœ¨ Enhanced prompt: {enhanced_prompt[:100]}...")
            
            # Step 2: Generate base audio with OBSIDIAN-Neural
            await self._ensure_generation_pipeline()
            
            base_audio, generation_info = await self.music_generator.generate_sample(
                musicgen_prompt=enhanced_prompt,
                tempo=bpm,
                generation_duration=duration,
                sample_rate=48000
            )
            
            # Step 3: Enhance with DJ Universe models (if enabled)
            if enhance_with_dj_models:
                enhanced_audio = await self._enhance_with_dj_models(
                    audio_data=base_audio,
                    bpm=bpm,
                    key=key,
                    genre=genre
                )
            else:
                enhanced_audio = base_audio
            
            # Step 4: Optimize for DJ use
            dj_optimized_audio = self._optimize_for_dj_use(
                audio_data=enhanced_audio,
                target_bpm=bpm,
                target_key=key
            )
            
            # Update statistics
            generation_time = (datetime.now() - start_time).total_seconds()
            self._update_generation_stats(generation_time, True)
            
            # Prepare metadata
            metadata = {
                "original_prompt": prompt,
                "enhanced_prompt": enhanced_prompt,
                "generation_time": generation_time,
                "bpm": bpm,
                "key": key,
                "genre": genre,
                "duration": duration,
                "enhanced_with_dj_models": enhance_with_dj_models,
                "generation_info": generation_info,
                "timestamp": start_time.isoformat()
            }
            
            logger.info(f"âœ… Music generated successfully in {generation_time:.2f}s")
            return dj_optimized_audio, metadata
            
        except Exception as e:
            logger.error(f"âŒ Music generation failed: {str(e)}")
            self._update_generation_stats(0, False)
            
            # Return silence as fallback
            silence = np.zeros(int(48000 * duration))
            error_metadata = {
                "error": str(e),
                "fallback": True,
                "duration": duration
            }
            return silence, error_metadata
    
    async def _enhance_with_dj_models(self, audio_data: np.ndarray, bpm: float,
                                    key: str, genre: str) -> np.ndarray:
        """
        Enhance audio using DJ Universe's 42+ AI models
        """
        try:
            # Use DJ Universe Master AI for enhancement
            enhancement_request = {
                "task": "audio_enhancement",
                "audio_features": {
                    "bpm": bpm,
                    "key": key,
                    "genre": genre,
                    "length": len(audio_data) / 48000
                },
                "enhancement_type": "dj_optimization"
            }
            
            # Send through DUMP protocol for ultra-fast processing
            dump_message = self.dj_master_ai.create_dump_message(
                component=0x04,  # AI_MODELS
                message_type=0x7,  # AI_QUERY
                data=json.dumps(enhancement_request).encode()
            )
            
            # Process with Master AI
            enhanced_features = await self.dj_master_ai.process_dump_message(dump_message)
            
            # Apply enhancements to audio
            enhanced_audio = self._apply_ai_enhancements(audio_data, enhanced_features)
            
            logger.info("âœ¨ Audio enhanced with DJ Universe models")
            return enhanced_audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ DJ model enhancement failed: {str(e)}, using original audio")
            return audio_data
    
    def _apply_ai_enhancements(self, audio_data: np.ndarray, 
                             enhanced_features: Dict[str, Any]) -> np.ndarray:
        """
        Apply AI-suggested enhancements to audio
        """
        # This would apply sophisticated audio enhancements
        # For now, we'll apply basic normalization and optimization
        
        # Normalize audio
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            normalized_audio = audio_data / max_val * 0.9
        else:
            normalized_audio = audio_data
        
        # Apply gentle compression for DJ use
        compressed_audio = np.tanh(normalized_audio * 1.2) * 0.8
        
        return compressed_audio
    
    def _optimize_for_dj_use(self, audio_data: np.ndarray, target_bpm: float,
                           target_key: str) -> np.ndarray:
        """
        Optimize audio specifically for DJ use
        """
        # Add gentle fade-in/fade-out for seamless looping
        sample_rate = 48000
        fade_duration = 0.1  # 100ms fade
        fade_samples = int(fade_duration * sample_rate)
        
        if len(audio_data) > fade_samples * 2:
            # Fade in
            fade_in = np.linspace(0, 1, fade_samples)
            audio_data[:fade_samples] *= fade_in
            
            # Fade out
            fade_out = np.linspace(1, 0, fade_samples)
            audio_data[-fade_samples:] *= fade_out
        
        # Ensure DC offset removal
        audio_data = audio_data - np.mean(audio_data)
        
        return audio_data
    
    async def analyze_audio_for_dj(self, audio_data: np.ndarray, 
                                 sample_rate: int = 48000) -> Dict[str, Any]:
        """
        Comprehensive audio analysis for DJ purposes
        """
        try:
            # Basic audio feature extraction
            audio_features = {
                "duration": len(audio_data) / sample_rate,
                "sample_rate": sample_rate,
                "channels": 1 if audio_data.ndim == 1 else audio_data.shape[0]
            }
            
            # Use librosa for advanced analysis
            if len(audio_data) > 0:
                # Tempo and beat analysis
                tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
                audio_features["bpm"] = float(tempo)
                audio_features["beat_times"] = beats.tolist()
                
                # Spectral features
                spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
                audio_features["spectral_centroid"] = float(np.mean(spectral_centroid))
                
                # Energy analysis
                rms_energy = librosa.feature.rms(y=audio_data)
                audio_features["energy_level"] = float(np.mean(rms_energy))
                
                # Key detection (basic)
                chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)
                key_profile = np.mean(chroma, axis=1)
                estimated_key = np.argmax(key_profile)
                key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
                audio_features["estimated_key"] = key_names[estimated_key]
            
            # Enhanced analysis using LLM
            llm_analysis = await self.obsidian_llm.analyze_track_for_dj(
                track_data={"duration": audio_features["duration"]},
                audio_features=audio_features
            )
            
            # Combine analyses
            comprehensive_analysis = {
                **audio_features,
                "llm_analysis": llm_analysis,
                "dj_recommendations": self._generate_dj_recommendations(audio_features),
                "timestamp": datetime.now().isoformat()
            }
            
            return comprehensive_analysis
            
        except Exception as e:
            logger.error(f"âŒ Audio analysis failed: {str(e)}")
            return {
                "error": str(e),
                "basic_info": {
                    "duration": len(audio_data) / sample_rate if len(audio_data) > 0 else 0,
                    "sample_rate": sample_rate
                }
            }
    
    def _generate_dj_recommendations(self, audio_features: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate DJ-specific recommendations based on audio features
        """
        bpm = audio_features.get("bpm", 120)
        energy = audio_features.get("energy_level", 0.5)
        
        # BPM categorization
        bpm_category = "medium"
        for category, (min_bpm, max_bpm) in self.bpm_ranges.items():
            if min_bpm <= bpm <= max_bpm:
                bpm_category = category
                break
        
        # Energy categorization
        energy_category = "low" if energy < 0.3 else "high" if energy > 0.7 else "medium"
        
        # Generate recommendations
        recommendations = {
            "bpm_category": bpm_category,
            "energy_category": energy_category,
            "suggested_mix_points": {
                "intro": "0-8 beats",
                "main": "8-24 beats",
                "outro": "24-32 beats"
            },
            "compatible_bpm_range": [max(60, bpm - 10), min(200, bpm + 10)],
            "suggested_effects": [],
            "crowd_energy": energy_category
        }
        
        # Effect suggestions based on energy and BPM
        if energy_category == "high":
            recommendations["suggested_effects"].extend(["reverb", "delay", "filter_sweep"])
        if bpm_category in ["fast", "ultra_fast"]:
            recommendations["suggested_effects"].extend(["gate", "bitcrusher"])
        if bpm_category == "slow":
            recommendations["suggested_effects"].extend(["reverb", "chorus"])
        
        return recommendations
    
    async def battle_performance_analysis(self, audio_stream: bytes,
                                        performance_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Real-time battle performance analysis
        """
        try:
            # Convert audio stream to numpy array
            audio_data = np.frombuffer(audio_stream, dtype=np.float32)
            
            # Analyze audio
            audio_analysis = await self.analyze_audio_for_dj(audio_data)
            
            # Get LLM-powered battle analysis
            battle_analysis = await self.llm_manager.query_for_battle_analysis(
                audio_features=audio_analysis,
                performance_data=performance_metadata
            )
            
            # Combine with DJ Universe AI models analysis
            dj_universe_scores = await self._get_dj_universe_battle_scores(
                audio_analysis, performance_metadata
            )
            
            # Final comprehensive analysis
            comprehensive_analysis = {
                "audio_analysis": audio_analysis,
                "llm_battle_analysis": battle_analysis,
                "dj_universe_scores": dj_universe_scores,
                "final_scores": self._calculate_final_battle_scores(
                    battle_analysis, dj_universe_scores
                ),
                "timestamp": datetime.now().isoformat()
            }
            
            return comprehensive_analysis
            
        except Exception as e:
            logger.error(f"âŒ Battle analysis failed: {str(e)}")
            return {
                "error": str(e),
                "fallback_scores": {
                    "technical_skill": 75.0,
                    "creativity": 75.0,
                    "crowd_engagement": 75.0
                }
            }
    
    async def _get_dj_universe_battle_scores(self, audio_analysis: Dict[str, Any],
                                           performance_metadata: Dict[str, Any]) -> Dict[str, float]:
        """
        Get battle scores from DJ Universe AI models
        """
        try:
            # Use competition models for evaluation
            evaluation_request = {
                "task": "battle_evaluation",
                "audio_features": audio_analysis,
                "performance_data": performance_metadata,
                "models_to_use": [
                    "ai_judge_technical",
                    "ai_judge_creative", 
                    "ai_judge_crowd",
                    "ai_judge_general"
                ]
            }
            
            # Send through DUMP protocol
            dump_message = self.dj_master_ai.create_dump_message(
                component=0x04,  # AI_MODELS
                message_type=0x7,  # AI_QUERY
                data=json.dumps(evaluation_request).encode()
            )
            
            # Get AI evaluation
            ai_response = await self.dj_master_ai.process_dump_message(dump_message)
            
            # Parse scores
            if isinstance(ai_response, dict) and "scores" in ai_response:
                return ai_response["scores"]
            else:
                # Fallback scores
                return {
                    "technical_skill": 80.0,
                    "creativity": 75.0,
                    "crowd_engagement": 78.0,
                    "mixing_quality": 82.0
                }
                
        except Exception as e:
            logger.warning(f"âš ï¸ DJ Universe battle scoring failed: {str(e)}")
            return {
                "technical_skill": 75.0,
                "creativity": 75.0,
                "crowd_engagement": 75.0,
                "mixing_quality": 75.0
            }
    
    def _calculate_final_battle_scores(self, llm_analysis: Dict[str, Any],
                                     dj_universe_scores: Dict[str, float]) -> Dict[str, float]:
        """
        Calculate weighted final scores from multiple AI analyses
        """
        # Weight the different analysis sources
        llm_weight = 0.3
        dj_universe_weight = 0.7
        
        final_scores = {}
        
        # Combine scores for each category
        for category in ["technical_skill", "creativity", "crowd_engagement", "mixing_quality"]:
            llm_score = llm_analysis.get(category, 75.0)
            dj_score = dj_universe_scores.get(category, 75.0)
            
            # Weighted average
            final_score = (llm_score * llm_weight) + (dj_score * dj_universe_weight)
            final_scores[category] = round(final_score, 1)
        
        # Overall score
        final_scores["overall"] = round(np.mean(list(final_scores.values())), 1)
        
        return final_scores
    
    def _update_generation_stats(self, generation_time: float, success: bool):
        """Update generation statistics"""
        self.generation_stats["total_generations"] += 1
        
        if success:
            self.generation_stats["successful_generations"] += 1
            
            # Update average generation time
            current_avg = self.generation_stats["avg_generation_time"]
            total_successful = self.generation_stats["successful_generations"]
            
            self.generation_stats["avg_generation_time"] = (
                (current_avg * (total_successful - 1) + generation_time) / total_successful
            )
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        stats = {
            "generation_stats": self.generation_stats,
            "model_cache_size": len(self.model_cache),
            "pipeline_ready": getattr(self, "_generation_pipeline_ready", False)
        }
        
        # Add LLM manager stats if available
        if self.llm_manager:
            stats["llm_stats"] = self.llm_manager.get_provider_stats()
        
        # Add OBSIDIAN stats if available
        if self.obsidian_llm:
            stats["obsidian_stats"] = self.obsidian_llm.get_session_stats()
        
        return stats
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check"""
        health = {
            "overall_status": "healthy",
            "components": {}
        }
        
        try:
            # Check LLM manager
            if self.llm_manager:
                llm_health = await self.llm_manager.health_check()
                health["components"]["llm_manager"] = llm_health
            
            # Check OBSIDIAN LLM
            if self.obsidian_llm:
                obsidian_health = await self.obsidian_llm.health_check()
                health["components"]["obsidian_llm"] = obsidian_health
            
            # Check DJ Master AI
            if self.dj_master_ai:
                health["components"]["dj_master_ai"] = {
                    "status": "healthy",
                    "models_loaded": len(self.dj_master_ai.loaded_models)
                }
            
            # Check generation pipeline
            health["components"]["generation_pipeline"] = {
                "status": "ready" if getattr(self, "_generation_pipeline_ready", False) else "not_initialized"
            }
            
            # Determine overall status
            component_statuses = []
            for component, status in health["components"].items():
                if isinstance(status, dict):
                    component_statuses.append(status.get("status", "unknown"))
                else:
                    component_statuses.append("unknown")
            
            if all(status in ["healthy", "ready"] for status in component_statuses):
                health["overall_status"] = "healthy"
            elif any(status in ["healthy", "ready"] for status in component_statuses):
                health["overall_status"] = "degraded"
            else:
                health["overall_status"] = "unhealthy"
                
        except Exception as e:
            health["overall_status"] = "error"
            health["error"] = str(e)
        
        return health

# Wrapper classes for OBSIDIAN-Neural components
class MusicGeneratorWrapper:
    """Wrapper for OBSIDIAN-Neural music generator"""
    
    async def initialize(self):
        """Initialize music generation model"""
        logger.info("ðŸŽµ Initializing music generator...")
        # This would initialize the actual Stable Audio model
        
    async def generate_sample(self, musicgen_prompt: str, tempo: float,
                            generation_duration: float, sample_rate: int) -> Tuple[np.ndarray, Dict]:
        """Generate music sample"""
        # For now, return silence as placeholder
        # In real implementation, this would use Stable Audio Open
        samples = int(generation_duration * sample_rate)
        audio_data = np.random.randn(samples) * 0.01  # Very quiet noise
        
        info = {
            "prompt": musicgen_prompt,
            "tempo": tempo,
            "duration": generation_duration,
            "sample_rate": sample_rate
        }
        
        return audio_data, info

class LayerManagerWrapper:
    """Wrapper for layer management"""
    pass

class StemsManagerWrapper:
    """Wrapper for stems separation"""
    pass

# Global instance
ai_dj_enhanced = AIDJEnhancedSystem()

async def get_ai_dj_enhanced() -> AIDJEnhancedSystem:
    """Get singleton AI-DJ Enhanced instance"""
    await ai_dj_enhanced.initialize()
    return ai_dj_enhanced